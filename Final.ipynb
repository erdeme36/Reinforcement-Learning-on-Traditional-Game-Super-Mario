{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd20148",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acd20148",
    "outputId": "c93cd542-bb55-47e8-87ca-edcaa2404a11"
   },
   "outputs": [],
   "source": [
    "!pip install nes-py==0.2.6\n",
    "!pip install gym-super-mario-bros\n",
    "!apt-get update\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y\n",
    "!apt install -y libgl1-mesa-glx\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0a093-48b7-40e8-b87b-bcbdc81097df",
   "metadata": {
    "id": "0ca0a093-48b7-40e8-b87b-bcbdc81097df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93100534-e915-44d3-8f8c-64a5a29c112e",
   "metadata": {
    "id": "93100534-e915-44d3-8f8c-64a5a29c112e"
   },
   "source": [
    "# RIGHT_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3977787-0ad7-4706-af58-61539df02559",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3977787-0ad7-4706-af58-61539df02559",
    "outputId": "3ac28cab-68bb-48ea-a54f-83c2cfc09c5e"
   },
   "outputs": [],
   "source": [
    "RIGHT_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe64b0b-8979-4fa6-95d9-20eb82050004",
   "metadata": {
    "id": "cfe64b0b-8979-4fa6-95d9-20eb82050004"
   },
   "source": [
    "# SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172fd257-c11a-4895-800d-101874c8498a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "172fd257-c11a-4895-800d-101874c8498a",
    "outputId": "74af59b8-76f1-4fd9-bb79-306454bfccc2"
   },
   "outputs": [],
   "source": [
    "SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59055f8b-1f23-4f1a-9e1c-3378d9e58f55",
   "metadata": {
    "id": "59055f8b-1f23-4f1a-9e1c-3378d9e58f55"
   },
   "source": [
    "# COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7d266-b67d-4c87-ab94-162e94fca4b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43f7d266-b67d-4c87-ab94-162e94fca4b0",
    "outputId": "0f56325f-e15b-48f2-bd26-f6e0244f7871"
   },
   "outputs": [],
   "source": [
    "COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74edfb-0e9c-461c-93a4-d6fafd8beba4",
   "metadata": {
    "id": "cb74edfb-0e9c-461c-93a4-d6fafd8beba4"
   },
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "        Each action of the agent is repeated over skip frames\n",
    "        return only every `skip`-th frame\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MarioRescale84x84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples/Rescales each frame to size 84x84 with greyscale\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(MarioRescale84x84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return MarioRescale84x84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\" \n",
    "        # image normalization on RBG\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Each frame is converted to PyTorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "    \n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Only every k-th frame is collected by the buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class PixelNormalization(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Normalize pixel values in frame --> 0 to 1\n",
    "    \"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def create_mario_env(env,movement):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = MarioRescale84x84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = PixelNormalization(env)\n",
    "    return JoypadSpace(env, movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c004452-de8e-4fb8-aa6f-a0320284b4b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c004452-de8e-4fb8-aa6f-a0320284b4b0",
    "outputId": "6c6c3265-ac47-4a82-b8d6-8fba2a6adbb6"
   },
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v1')\n",
    "env = create_mario_env(env,RIGHT_ONLY) \n",
    "# Observation Space\n",
    "observation_space = env.observation_space.shape\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953b9a5-36ab-4f60-a026-57007b0110f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e953b9a5-36ab-4f60-a026-57007b0110f6",
    "outputId": "746a48cf-75b7-4825-9c7c-1dc5f2ab0d3e"
   },
   "outputs": [],
   "source": [
    "# Action Space\n",
    "env = create_mario_env(env,RIGHT_ONLY) \n",
    "action_space = env.action_space.n\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b3633-7a16-4841-b672-8f5227547b96",
   "metadata": {
    "id": "489b3633-7a16-4841-b672-8f5227547b96"
   },
   "outputs": [],
   "source": [
    "model =nn.Sequential(\n",
    "    nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "\n",
    "    nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "\n",
    "    nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256*4*4,1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cc546-0d4f-4192-85c3-21e1d06f33f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d7cc546-0d4f-4192-85c3-21e1d06f33f9",
    "outputId": "d451367a-fa66-400b-c3c4-1b862907e68e"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ea34d-8c1d-4e41-9e85-ef0e893d0305",
   "metadata": {
    "id": "2d1ea34d-8c1d-4e41-9e85-ef0e893d0305"
   },
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25191376-693b-421b-a986-319de1d34f38",
   "metadata": {
    "id": "25191376-693b-421b-a986-319de1d34f38"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    # Initializers\n",
    "    def __init__(self, state_space, action_space, batch_size, gamma, lr, dropout, exploration_max,\n",
    "                       exploration_min, pretrained, exploration_decay, double_dqn = False, max_memory_size = 30000):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.pretrained = pretrained\n",
    "        self.memory_sample_size = batch_size\n",
    "\n",
    "        # check cuda or cpu\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            print(\"Current device is Cuda\")    \n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            print(\"Current device is cpu\") \n",
    "        self.dqn = DQNSolver(state_space, action_space).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "        self.max_memory_size = max_memory_size\n",
    "        # In order to save pretarined to move on fast \n",
    "        if self.pretrained:\n",
    "          # Loading Pretrained Project Files\n",
    "            self.dqn.load_state_dict(torch.load(\"DQN.pt\", map_location=torch.device(self.device)))\n",
    "            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n",
    "            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n",
    "            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n",
    "            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n",
    "            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n",
    "            with open(\"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "\n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.CrossEntropyLoss().to(self.device) # Cross Entropy Loss\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        \"\"\"Store the experiences in a buffer to use later\"\"\"\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "    \n",
    "    def batch_experiences(self):\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]      \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        else:\n",
    "            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "    \n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "        # Sample a batch of experiences\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.batch_experiences()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        # Implementation of DQN \n",
    "        # Formula ==> Q*(S, A) <= r + γ max_a Q(S', a) \n",
    "        target = REWARD + torch.mul((self.gamma * self.dqn(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "            \n",
    "        current = self.dqn(STATE).gather(1, ACTION.long())\n",
    "        loss = self.l1(current, target) # x, y values for the loss function\n",
    "        loss.backward() # Compute gradients\n",
    "\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oyRVGw_oV0MW",
   "metadata": {
    "id": "oyRVGw_oV0MW"
   },
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "\n",
    "    def __init__(self, state_space, action_space, batch_size, gamma, lr, dropout, exploration_max, exploration_min,\n",
    "                       exploration_decay, pretrained, double_dqn = True, max_memory_size = 30000):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.double_dqn = double_dqn\n",
    "        self.pretrained = pretrained\n",
    "        self.memory_sample_size = batch_size\n",
    "        self.max_memory_size = max_memory_size\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            print(\"Current Device is Cuda\")    \n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            print(\"Current Device is cpu\") \n",
    "        # Init Double DQN network\n",
    "        self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "        self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "        \n",
    "        if self.pretrained:\n",
    "            self.local_net.load_state_dict(torch.load(\"DQN1.pt\", map_location=torch.device(self.device)))\n",
    "            self.target_net.load_state_dict(torch.load(\"DQN2.pt\", map_location=torch.device(self.device)))\n",
    "        # Optimizer is Adam\n",
    "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "        self.copy = 100\n",
    "        self.step = 0\n",
    "\n",
    "        # In order to save pretarined to move on fast\n",
    "        if self.pretrained:\n",
    "          # Loading Pretrained Project Files\n",
    "            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n",
    "            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n",
    "            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n",
    "            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n",
    "            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n",
    "            with open(\"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "         \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.CrossEntropyLoss().to(self.device)\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "    \n",
    "    def batch_experiences(self):\n",
    "        idx    = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        STATE  = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE   = self.DONE_MEM[idx]      \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.double_dqn:\n",
    "            self.step += 1\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        if self.double_dqn:\n",
    "            return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.double_dqn and self.step % self.copy == 0:\n",
    "            self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "        # taking state action reward second state and done from batch experiences\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.batch_experiences()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "\n",
    "        # Implementation of DQN \n",
    "        # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "        target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)),  1 - DONE)\n",
    "        current = self.local_net(STATE).gather(1, ACTION.long())\n",
    "\n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e8e70-e0c6-4b20-99fa-59471e36327d",
   "metadata": {
    "id": "786e8e70-e0c6-4b20-99fa-59471e36327d"
   },
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a76cca-8634-47ba-b982-de2d36b431fc",
   "metadata": {
    "id": "75a76cca-8634-47ba-b982-de2d36b431fc"
   },
   "outputs": [],
   "source": [
    "def run(training_mode, pretrained, double_dqn, exploration_max=1, movement = RIGHT_ONLY, num_episodes = 500):\n",
    "    # Right Only Default    \n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "    env = create_mario_env(env,movement)\n",
    "    if double_dqn == False:\n",
    "      agent = DQN(state_space = observation_space,\n",
    "                  action_space = action_space,\n",
    "                  batch_size = 32,\n",
    "                  gamma = 0.85,\n",
    "                  lr = 25e-3,\n",
    "                  dropout = 0.2,\n",
    "                  exploration_max = 1.0,\n",
    "                  exploration_min = 0.02,\n",
    "                  exploration_decay = 0.99,\n",
    "                  double_dqn = double_dqn,\n",
    "                  pretrained = pretrained)\n",
    "    else:\n",
    "      agent = DDQN(state_space = observation_space,\n",
    "                   action_space = action_space,\n",
    "                   batch_size = 32,\n",
    "                   gamma = 0.85,\n",
    "                   lr = 25e-3,\n",
    "                   dropout = 0.2,\n",
    "                   exploration_max = 1.0,\n",
    "                   exploration_min = 0.02,\n",
    "                   exploration_decay = 0.99,\n",
    "                   double_dqn = double_dqn,\n",
    "                   pretrained = pretrained)\n",
    "    # Restart the enviroment for each episode\n",
    "    num_episodes = num_episodes\n",
    "    env.reset()\n",
    "    \n",
    "    total_rewards = []\n",
    "    average_rewards = []\n",
    "    \n",
    "    if training_mode and pretrained:\n",
    "        with open(\"total_rewards.pkl\", 'rb') as f:\n",
    "            total_rewards = pickle.load(f)\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num + 1)\n",
    "            action = agent.act(state)\n",
    "            steps += 1\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, action, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        average_rewards.append(np.mean(total_rewards))\n",
    "\n",
    "        print(\"=> Episode {} Score = {:.2f}, Average Score = {:.2f}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "\n",
    "    if training_mode:\n",
    "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.ending_position, f)\n",
    "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.num_in_queue, f)\n",
    "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
    "            pickle.dump(total_rewards, f)\n",
    "        if agent.double_dqn:\n",
    "            torch.save(agent.local_net.state_dict(), \"DQN1.pt\")\n",
    "            torch.save(agent.target_net.state_dict(), \"DQN2.pt\")\n",
    "        else:\n",
    "            torch.save(agent.dqn.state_dict(), \"DQN.pt\")  \n",
    "        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n",
    "        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n",
    "        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n",
    "        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n",
    "        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n",
    "    # plot Episodes vs Reward\n",
    "    x = np.arange(1,len(average_rewards)+1)\n",
    "    y = average_rewards   \n",
    "    return(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059b086-433c-42b8-aab3-b0223c9cb282",
   "metadata": {
    "id": "b059b086-433c-42b8-aab3-b0223c9cb282"
   },
   "source": [
    "## Deep Q-Learning Movement => Right Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c98f2-4b0c-4c3b-8b4a-c066979a1e9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "448c98f2-4b0c-4c3b-8b4a-c066979a1e9e",
    "outputId": "8b8bd5a5-9058-44b3-9b27-4f1a4242d0fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episode that you want to train;\n",
    "episodes_nb = 2\n",
    "# For training\n",
    "deepq_right_x, deepq_right_y = run(training_mode=True, pretrained=False, double_dqn=False, num_episodes=episodes_nb, \n",
    "                                   exploration_max = 0.95, movement = RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae76a41-88d1-4c23-9d13-16da3ca42946",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "2ae76a41-88d1-4c23-9d13-16da3ca42946",
    "outputId": "b05acd9f-b944-47c7-db96-00a1f0a97fd1"
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "episodes_nb_test = 3\n",
    "deepq_right_x_test,deepq_right_y_test = run(training_mode=False, pretrained=False, double_dqn=False, num_episodes=episodes_nb_test, \n",
    "                                            exploration_max = 0.05, movement = RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5aed92-48c5-428a-8087-8c797f9c960f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "fd5aed92-48c5-428a-8087-8c797f9c960f",
    "outputId": "8aa9e015-d512-4590-9e0c-390774da10c8"
   },
   "outputs": [],
   "source": [
    "# Train Plot\n",
    "plt.plot(deepq_right_x, deepq_right_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - RIGHT_ONLY)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_60aIrUfEeo5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "_60aIrUfEeo5",
    "outputId": "d60847d9-2541-430d-b4af-509cd842482e"
   },
   "outputs": [],
   "source": [
    "# Test Plot\n",
    "plt.plot(deepq_right_x_test, deepq_right_y_test)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - RIGHT_ONLY)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ebad8-58f8-4d27-9076-139f9c65e490",
   "metadata": {
    "id": "d75ebad8-58f8-4d27-9076-139f9c65e490",
    "tags": []
   },
   "source": [
    "## Deep Q-Learning Movement => SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e65360-73b5-4920-8754-93dac185456c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02e65360-73b5-4920-8754-93dac185456c",
    "outputId": "f33462e6-cd2a-40a7-8e3a-ed4a68f20077",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episode that you want to train;\n",
    "#episodes_nb = 3\n",
    "# For training\n",
    "deepq_simple_x, deepq_simple_y = run(training_mode=True, pretrained=False, double_dqn=False, num_episodes=episodes_nb, \n",
    "                                     exploration_max = 0.95, movement = SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d47dcf-1c6f-46ed-8ac9-23d575491a0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "86d47dcf-1c6f-46ed-8ac9-23d575491a0c",
    "outputId": "94332340-bde7-454d-d381-6d11299b7dc3"
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "#episodes_nb_test = 3\n",
    "deepq_simple_x_test,deepq_simple_y_test = run(training_mode=False, pretrained=False, double_dqn=False, num_episodes=episodes_nb_test, \n",
    "                                              exploration_max = 0.05, movement = SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b38c9-5e1f-4333-8ab3-ffeaebc64df0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e06b38c9-5e1f-4333-8ab3-ffeaebc64df0",
    "outputId": "093e190c-a6fd-4718-d86b-5f4c3305fc96"
   },
   "outputs": [],
   "source": [
    "plt.plot(deepq_simple_x, deepq_simple_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - SIMPLE_MOVEMENT)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413c7d7-6d2b-4cb8-99ba-d5fd1302277b",
   "metadata": {
    "id": "3413c7d7-6d2b-4cb8-99ba-d5fd1302277b",
    "tags": []
   },
   "source": [
    "## Deep Q-Learning Movement => COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb3b42-073e-496a-beb7-11b8386fe788",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cfb3b42-073e-496a-beb7-11b8386fe788",
    "outputId": "c10909a0-8608-4f81-dda6-2637f500778e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episode that you want to train;\n",
    "#episodes_nb = 3\n",
    "# For training\n",
    "deepq_complex_x, deepq_complex_y = run(training_mode=True, pretrained=False, double_dqn=False, num_episodes=episodes_nb, \n",
    "                                       exploration_max = 0.95, movement = COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17d805-c05c-4e63-8268-79c04dd34219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "dd17d805-c05c-4e63-8268-79c04dd34219",
    "outputId": "576fc335-baba-4797-833c-3531ce5bd1b2"
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "#episodes_nb_test = 3\n",
    "deepq_complex_x_test,deepq_complex_y_test = run(training_mode=False, pretrained=False, double_dqn=False, num_episodes=episodes_nb_test, \n",
    "                                                exploration_max = 0.05, movement = COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790301fb-a87d-4133-93a2-703e375980cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "790301fb-a87d-4133-93a2-703e375980cf",
    "outputId": "3e4dae1e-f100-4fc2-e581-2278b1cc54c1"
   },
   "outputs": [],
   "source": [
    "plt.plot(deepq_complex_x, deepq_complex_y_test)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - COMPLEX_MOVEMENT)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0b4b5-b27d-4110-8a36-2c60491bd068",
   "metadata": {
    "id": "10a0b4b5-b27d-4110-8a36-2c60491bd068"
   },
   "source": [
    "## Double Deep Q-Learning Movement => Right Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8026bd-3411-4e54-98ba-02dfb8ad4535",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "8a8026bd-3411-4e54-98ba-02dfb8ad4535",
    "outputId": "b7b79e48-21af-4826-b8e1-dc317d4e7e24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episode that you want to train;\n",
    "#episodes_nb = 3\n",
    "# For training\n",
    "double_deepq_right_x, double_deepq_right_y = run(training_mode=True, pretrained=False, double_dqn=True, num_episodes=episodes_nb, \n",
    "                                                 exploration_max = 0.95, movement = RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61246e07-4fea-4bf7-854a-eb64724b5fb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "61246e07-4fea-4bf7-854a-eb64724b5fb4",
    "outputId": "54b3e8ca-68b6-47ee-b47f-53a765c9aeff"
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "#episodes_nb_test = 3\n",
    "double_deepq_right_x_test,double_deepq_right_y_test = run(training_mode=False, pretrained=False, double_dqn=True, num_episodes=episodes_nb_test, \n",
    "                                                          exploration_max = 0.05, movement = RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4b9d4-d259-4e24-8477-d1ff79cc62b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "d6d4b9d4-d259-4e24-8477-d1ff79cc62b0",
    "outputId": "7c62146e-7088-40c3-99ce-4015cfc0e9f8"
   },
   "outputs": [],
   "source": [
    "plt.plot(double_deepq_right_x, double_deepq_right_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - RIGHT_ONLY)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16426e85-98f3-43c0-9d52-ab921250524f",
   "metadata": {
    "id": "16426e85-98f3-43c0-9d52-ab921250524f",
    "tags": []
   },
   "source": [
    "## Double Deep Q-Learning Movement => SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1d70f-bf26-4013-bb50-6510e818aaee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0a1d70f-bf26-4013-bb50-6510e818aaee",
    "outputId": "9612008d-5390-4e12-b0f4-f775db29ff0f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episode that you want to train;\n",
    "#episodes_nb = 3\n",
    "# For training\n",
    "double_deepq_simple_x, double_deepq_simple_y = run(training_mode=True, pretrained=False, double_dqn=True, num_episodes=episodes_nb, \n",
    "                                                   exploration_max = 0.95, movement = SIMPLE_MOVEMENT)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a43970-f84d-42a9-a43a-016060be2537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "86a43970-f84d-42a9-a43a-016060be2537",
    "outputId": "ebef63c3-c21b-4ebe-8da5-65f1767f2d5b"
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "#episodes_nb = 3\n",
    "double_deepq_simple_x_test,double_deepq_simple_y_test  = run(training_mode=False, pretrained=False, double_dqn=True, num_episodes=episodes_nb_test, \n",
    "                                                             exploration_max = 0.05, movement = SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14add729-9a66-41fa-878c-316852476b79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "14add729-9a66-41fa-878c-316852476b79",
    "outputId": "cdd9a965-0fb1-47ee-ae36-0f8e15875ced"
   },
   "outputs": [],
   "source": [
    "plt.plot(double_deepq_simple_x, double_deepq_simple_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - SIMPLE_MOVEMENT)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9569b49-254a-464a-bfaa-2247ed7647f4",
   "metadata": {
    "id": "f9569b49-254a-464a-bfaa-2247ed7647f4",
    "tags": []
   },
   "source": [
    "## Double Deep Q-Learning Movement => COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392041d6-2c84-4120-a520-6ea9bd35db9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "392041d6-2c84-4120-a520-6ea9bd35db9e",
    "outputId": "ffe7b5de-66b5-45e1-8985-692682475915",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episode that you want to train;\n",
    "#episodes_nb = 3\n",
    "# For training\n",
    "double_deepq_complex_x, double_deepq_complex_y = run(training_mode=True, pretrained=False, double_dqn=True, num_episodes=episodes_nb, \n",
    "                                                     exploration_max = 0.95, movement = COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ff1c8-c710-463d-8ec7-6289318523cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "794ff1c8-c710-463d-8ec7-6289318523cf",
    "outputId": "77625ddb-88b9-4e5e-8f26-a6d8ad16c5a0"
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "#episodes_nb = 3\n",
    "double_deepq_complex_x_test,double_deepq_complex_y_test  = run(training_mode=False, pretrained=False, double_dqn=True, num_episodes=episodes_nb_test, \n",
    "                                                               exploration_max = 0.05, movement = COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c56d9f-9ab3-405b-bded-77c728f85b44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "17c56d9f-9ab3-405b-bded-77c728f85b44",
    "outputId": "6aea34fb-b354-4c8b-bac3-e497c45cfd01"
   },
   "outputs": [],
   "source": [
    "plt.plot(double_deepq_complex_x, double_deepq_complex_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q - COMPLEX_MOVEMENT)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PwtqOY21I08w",
   "metadata": {
    "id": "PwtqOY21I08w"
   },
   "source": [
    "Comparison Deep Q vs Double Deep Q for Right Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dAx9atCHJCEc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "dAx9atCHJCEc",
    "outputId": "40a03478-8ce2-4066-8238-af48cc296e64"
   },
   "outputs": [],
   "source": [
    "plt.plot(deepq_right_x, deepq_right_y)\n",
    "plt.plot(double_deepq_right_x, double_deepq_right_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q vs Doble Deep Q - RIGHT_ONLY)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.legend([\"DQN\",\"DDQN\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9wQ8AEP2JA8J",
   "metadata": {
    "id": "9wQ8AEP2JA8J"
   },
   "source": [
    "Comparison Deep Q vs Double Deep Q for Simple Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4c177-39ed-4fc7-83fa-1183f6113d8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "c3d4c177-39ed-4fc7-83fa-1183f6113d8d",
    "outputId": "28609342-c38a-4198-c3df-c1b65abb4b30"
   },
   "outputs": [],
   "source": [
    "plt.plot(deepq_simple_x, deepq_simple_y)\n",
    "plt.plot(double_deepq_simple_x, double_deepq_simple_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q vs Doble Deep Q - SIMPLE_MOVEMENT)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.legend([\"DQN\",\"DDQN\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dSohBse1BlEk",
   "metadata": {
    "id": "dSohBse1BlEk"
   },
   "source": [
    "Comparison Deep Q vs Double Deep Q for Complex Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a50bf-7fc8-451c-ae39-85ece1f356b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "122a50bf-7fc8-451c-ae39-85ece1f356b5",
    "outputId": "730b6a8b-3387-458d-d77b-cb20db6a1fc1"
   },
   "outputs": [],
   "source": [
    "plt.plot(deepq_right_x, deepq_right_y)\n",
    "plt.plot(double_deepq_complex_x, double_deepq_complex_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q vs Doble Deep Q - COMPLEX_MOVEMENT)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.legend([\"DQN\",\"DDQN\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x1a79UyFBmIh",
   "metadata": {
    "id": "x1a79UyFBmIh"
   },
   "source": [
    "Comparison Deep Q for Each Action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209b0c3-6f0e-4845-8603-60a020b2c9f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "1209b0c3-6f0e-4845-8603-60a020b2c9f9",
    "outputId": "5fbfa1aa-3bdd-4e54-8e5d-08d8fb2bc935"
   },
   "outputs": [],
   "source": [
    "plt.plot(deepq_right_x, deepq_right_y)\n",
    "plt.plot(deepq_simple_x, deepq_simple_y)\n",
    "plt.plot(deepq_complex_x, deepq_complex_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Deep Q)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.grid()\n",
    "plt.legend([\"RIGHT_ONLY\",\"SIMPLE_MOVEMENT\",\"COMPLEX_MOVEMENT\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sC1CH66ODCKv",
   "metadata": {
    "id": "sC1CH66ODCKv"
   },
   "source": [
    "Comparison Double Deep Q for Each Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20F6hLd4DBDM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "20F6hLd4DBDM",
    "outputId": "d42ed0bd-0abe-4512-f0dd-4095f73912d2"
   },
   "outputs": [],
   "source": [
    "plt.plot(double_deepq_right_x, double_deepq_right_y)\n",
    "plt.plot(double_deepq_simple_x, double_deepq_simple_y)\n",
    "plt.plot(double_deepq_complex_x, double_deepq_complex_y)\n",
    "plt.title(f\"Episodes vs Average Rewards (Double Deep Q)\")\n",
    "plt.xlabel(\"Episodes\") \n",
    "plt.ylabel(\"Average Rewards\") \n",
    "plt.legend([\"RIGHT_ONLY\",\"SIMPLE_MOVEMENT\",\"COMPLEX_MOVEMENT\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a8bd7-b6f0-4f51-b0db-9f43c0cbb54f",
   "metadata": {
    "id": "573a8bd7-b6f0-4f51-b0db-9f43c0cbb54f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
